{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYJuLuZqG77k",
        "outputId": "0ac90a26-c23e-44a4-a5bd-3a5fc1b484c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn xgboost pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/abhinavuser/UAV-UAS /content/repo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e89PF3KrHZwZ",
        "outputId": "09b144cf-6836-4aeb-cbd1-93a62ddc7e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/repo'...\n",
            "remote: Enumerating objects: 183076, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 183076 (delta 1), reused 18 (delta 1), pack-reused 183057 (from 1)\u001b[K\n",
            "Receiving objects: 100% (183076/183076), 103.31 MiB | 26.01 MiB/s, done.\n",
            "Resolving deltas: 100% (145612/145612), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textwrap import dedent\n",
        "from pathlib import Path\n",
        "\n",
        "base = Path(\"/content\")\n",
        "\n",
        "generate_dataset_py = dedent(\"\"\"\n",
        "#!/usr/bin/env python3\n",
        "\\\"\\\"\\\"\n",
        "generate_dataset.py\n",
        "----------------------------------\n",
        "Builds a *realistic* coverage dataset from a source repo using:\n",
        "- Heuristics (always available)\n",
        "- Optional openâ€‘source scanners if installed: hadolint, trivy, osv-scanner, semgrep, radon\n",
        "\n",
        "Each row in the CSV corresponds to an artifact (Dockerfile, dependency manifest, or code module).\n",
        "It outputs the schema your ML expects:\n",
        "[name, module, priority, risk_score, complexity_score, status, business_impact]\n",
        "\n",
        "USAGE:\n",
        "  python generate_dataset.py --repo /path/to/repo --out coverage_dataset.csv\n",
        "  # Optional: --prefer-tools to use CLI scanners when available\n",
        "  # Optional: --llm (experimental) to summarize risks if `ollama` is installed\n",
        "\n",
        "Install suggestions (optional):\n",
        "  hadolint, trivy, osv-scanner, semgrep, radon, gitleaks, trufflehog, ollama\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "import argparse\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "# ----------------------------\n",
        "# Utilities\n",
        "# ----------------------------\n",
        "\n",
        "def which(cmd: str) -> bool:\n",
        "    from shutil import which as _which\n",
        "    return _which(cmd) is not None\n",
        "\n",
        "def run(cmd):\n",
        "    try:\n",
        "        res = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=False)\n",
        "        return res.returncode, res.stdout.strip(), res.stderr.strip()\n",
        "    except Exception as e:\n",
        "        return -1, \"\", str(e)\n",
        "\n",
        "def top_level_module(path: Path, repo: Path) -> str:\n",
        "    try:\n",
        "        rel = path.relative_to(repo)\n",
        "    except Exception:\n",
        "        rel = path\n",
        "    parts = rel.parts\n",
        "    return parts[0] if parts else \"root\"\n",
        "\n",
        "def load_text(path: Path) -> str:\n",
        "    try:\n",
        "        return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "# ----------------------------\n",
        "# Scoring helpers\n",
        "# ----------------------------\n",
        "\n",
        "PRIORITY_KEYWORDS = {\n",
        "    \"high\": [\"auth\", \"payment\", \"payments\", \"security\", \"infra\", \"gateway\", \"checkout\"],\n",
        "    \"medium\": [\"admin\", \"product\", \"order\", \"catalog\", \"search\", \"inventory\"],\n",
        "    \"low\": [\"ui\", \"docs\", \"styles\", \"examples\", \"demo\", \"sample\"]\n",
        "}\n",
        "\n",
        "IMPACT_KEYWORDS = {\n",
        "    \"Critical\": [\"payment\", \"payments\", \"auth\", \"security\", \"secrets\", \"keys\", \"pii\"],\n",
        "    \"High\": [\"checkout\", \"orders\", \"infra\", \"gateway\"],\n",
        "    \"Medium\": [\"product\", \"catalog\", \"search\", \"email\"],\n",
        "    \"Low\": [\"docs\", \"ui\", \"theme\"]\n",
        "}\n",
        "\n",
        "def infer_priority(name: str, module: str) -> str:\n",
        "    text = f\\\"{name} {module}\\\".lower()\n",
        "    for level, kws in PRIORITY_KEYWORDS.items():\n",
        "        if any(kw in text for kw in kws):\n",
        "            return level\n",
        "    return \"medium\"\n",
        "\n",
        "def infer_impact(name: str, module: str) -> str:\n",
        "    text = f\\\"{name} {module}\\\".lower()\n",
        "    for level, kws in IMPACT_KEYWORDS.items():\n",
        "        if any(kw in text for kw in kws):\n",
        "            return level\n",
        "    # Default a bit conservative\n",
        "    return \"Medium\"\n",
        "\n",
        "def normalize_1_to_5(value, max_value, fallback=1.0):\n",
        "    try:\n",
        "        if max_value <= 0:\n",
        "            return fallback\n",
        "        x = 5.0 * float(value) / float(max_value)\n",
        "        return max(1.0, min(5.0, x))\n",
        "    except Exception:\n",
        "        return fallback\n",
        "\n",
        "# Risk = f(vulns, lint findings, secrets, outdated deps)\n",
        "def combine_risk(vuln_score, lint_score, secret_score, misc_score) -> float:\n",
        "    # logarithmic dampening, then scale to 1-5\n",
        "    total = vuln_score*1.2 + lint_score*0.8 + secret_score*1.5 + misc_score*0.7\n",
        "    return min(5.0, 1.0 + math.log(1.0 + total + 1e-6, 2))\n",
        "\n",
        "# Complexity = f(code size, cyclomatic complexity if available)\n",
        "def combine_complexity(sloc, cc_avg) -> float:\n",
        "    # heuristics: sloc contributes, cc adds a bit\n",
        "    sloc_part = min(5.0, 1.0 + math.log(1 + sloc, 5))\n",
        "    if cc_avg is None:\n",
        "        return sloc_part\n",
        "    return min(5.0, 0.7*sloc_part + 0.3*min(5.0, 1.0 + math.log(1 + cc_avg, 2)))\n",
        "\n",
        "def infer_status(tests_count: int, sloc: int) -> str:\n",
        "    if tests_count == 0:\n",
        "        return \"uncovered\"\n",
        "    ratio = tests_count / max(10, sloc/50)  # rough proxy\n",
        "    if ratio >= 1.0:\n",
        "        return \"covered\"\n",
        "    if ratio >= 0.3:\n",
        "        return \"partial\"\n",
        "    return \"uncovered\"\n",
        "\n",
        "# ----------------------------\n",
        "# Heuristic analyzers (no external tools)\n",
        "# ----------------------------\n",
        "\n",
        "DOCKER_ISSUE_PATTERNS = [\n",
        "    (r\\\"FROM .*:latest\\\", 5, \"Using latest tag\"),\n",
        "    (r\\\"^USER root\\\", 4, \"Running as root\"),\n",
        "    (r\\\"apk add .* --no-cache\\\" , 1, \"apk add ok (no-cache)\"),\n",
        "    (r\\\"apt-get install .* -y\\\", 2, \"apt-get install potentially caching\"),\n",
        "    (r\\\"curl .*\\\\|\\\\s*sh\\\", 5, \"Curl pipe to shell\"),\n",
        "    (r\\\"ADD \\\", 2, \"Prefer COPY over ADD\"),\n",
        "]\n",
        "\n",
        "def analyze_dockerfile_heuristic(text: str):\n",
        "    vulns = 0\n",
        "    for pat, weight, _desc in DOCKER_ISSUE_PATTERNS:\n",
        "        if re.search(pat, text, flags=re.MULTILINE):\n",
        "            vulns += weight\n",
        "    return vulns\n",
        "\n",
        "def sloc_count(text: str) -> int:\n",
        "    return sum(1 for line in text.splitlines() if line.strip())\n",
        "\n",
        "def guess_tests_for_path(path: Path, repo: Path) -> int:\n",
        "    # crude heuristic: count files under tests/ that mention the module name\n",
        "    tests_root = repo / \"tests\"\n",
        "    if not tests_root.exists():\n",
        "        return 0\n",
        "    name = top_level_module(path, repo).lower()\n",
        "    count = 0\n",
        "    for root, _, files in os.walk(tests_root):\n",
        "        for f in files:\n",
        "            p = Path(root) / f\n",
        "            try:\n",
        "                t = p.read_text(encoding=\"utf-8\", errors=\"ignore\").lower()\n",
        "                if name in t:\n",
        "                    count += 1\n",
        "            except Exception:\n",
        "                pass\n",
        "    return count\n",
        "\n",
        "# ----------------------------\n",
        "# Optional tool-based analyzers\n",
        "# ----------------------------\n",
        "\n",
        "def hadolint_findings(path: Path) -> int:\n",
        "    if not which(\"hadolint\"):\n",
        "        return 0\n",
        "    code, out, _ = run([\"hadolint\", \"-f\", \"json\", str(path)])\n",
        "    if code != 0 or not out:\n",
        "        return 0\n",
        "    try:\n",
        "        arr = json.loads(out)\n",
        "        # weight by severity\n",
        "        sev_w = {\"error\": 4, \"warning\": 2, \"info\": 1, \"style\": 1}\n",
        "        score = sum(sev_w.get(item.get(\"level\",\"info\").lower(),1) for item in arr)\n",
        "        return score\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "def trivy_fs_findings(repo: Path) -> int:\n",
        "    if not which(\"trivy\"):\n",
        "        return 0\n",
        "    code, out, _ = run([\"trivy\", \"fs\", \"--quiet\", \"--format\", \"json\", str(repo)])\n",
        "    if code != 0 or not out:\n",
        "        return 0\n",
        "    try:\n",
        "        obj = json.loads(out)\n",
        "        score = 0\n",
        "        sev_w = {\"CRITICAL\": 8, \"HIGH\": 5, \"MEDIUM\": 3, \"LOW\": 1, \"UNKNOWN\": 1}\n",
        "        for res in obj.get(\"Results\", []):\n",
        "            for v in res.get(\"Vulnerabilities\", []) if res.get(\"Vulnerabilities\") else []:\n",
        "                score += sev_w.get(v.get(\"Severity\",\"LOW\").upper(), 1)\n",
        "        return score\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "def osv_findings(repo: Path) -> int:\n",
        "    if not which(\"osv-scanner\"):\n",
        "        return 0\n",
        "    code, out, _ = run([\"osv-scanner\", \"--json\", str(repo)])\n",
        "    if code != 0 or not out:\n",
        "        return 0\n",
        "    try:\n",
        "        obj = json.loads(out)\n",
        "        score = 0\n",
        "        sev_w = {\"CRITICAL\": 8, \"HIGH\": 5, \"MODERATE\": 3, \"MEDIUM\": 3, \"LOW\": 1}\n",
        "        for r in obj.get(\"results\", []):\n",
        "            for p in r.get(\"packages\", []):\n",
        "                for v in p.get(\"vulnerabilities\", []):\n",
        "                    sev = v.get(\"severity\", [])\n",
        "                    # try to read a numeric CVSS if present\n",
        "                    cvss_max = 0.0\n",
        "                    for s in sev:\n",
        "                        # GHSA style might have score\n",
        "                        try:\n",
        "                            sc = float(s.get(\"score\", 0))\n",
        "                            cvss_max = max(cvss_max, sc)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                    if cvss_max >= 9.0:\n",
        "                        score += 8\n",
        "                    elif cvss_max >= 7.0:\n",
        "                        score += 5\n",
        "                    elif cvss_max >= 4.0:\n",
        "                        score += 3\n",
        "                    elif cvss_max > 0.0:\n",
        "                        score += 1\n",
        "        return score\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "def semgrep_findings(repo: Path) -> int:\n",
        "    if not which(\"semgrep\"):\n",
        "        return 0\n",
        "    code, out, _ = run([\"semgrep\", \"--json\", \"--quiet\", \"--error\", \"--severity\", \"WARNING\", str(repo)])\n",
        "    if code != 0 or not out:\n",
        "        return 0\n",
        "    try:\n",
        "        obj = json.loads(out)\n",
        "        score = 0\n",
        "        for r in obj.get(\"results\", []):\n",
        "            sev = r.get(\"extra\", {}).get(\"severity\",\"WARNING\").upper()\n",
        "            score += {\"ERROR\":4, \"WARNING\":2, \"INFO\":1}.get(sev, 1)\n",
        "        return score\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "def radon_cc_avg(repo: Path, module_name: str) -> float | None:\n",
        "    if not which(\"radon\"):\n",
        "        return None\n",
        "    # radon cc -s -j <path>\n",
        "    path = repo / module_name\n",
        "    if not path.exists():\n",
        "        return None\n",
        "    code, out, _ = run([\"radon\", \"cc\", \"-s\", \"-j\", str(path)])\n",
        "    if code != 0 or not out:\n",
        "        return None\n",
        "    try:\n",
        "        obj = json.loads(out)\n",
        "        totals = 0\n",
        "        count = 0\n",
        "        for _, entries in obj.items():\n",
        "            for e in entries:\n",
        "                try:\n",
        "                    totals += int(e.get(\"complexity\", 0))\n",
        "                    count += 1\n",
        "                except Exception:\n",
        "                    pass\n",
        "        return (totals / count) if count else None\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "# ----------------------------\n",
        "# (Optional) LLM summarizer\n",
        "# ----------------------------\n",
        "def ollama_summarize(text: str) -> str | None:\n",
        "    if not which(\"ollama\"):\n",
        "        return None\n",
        "    prompt = (\n",
        "        \"You are a security auditor. Read the following Dockerfile or code snippet \"\n",
        "        \"and list 3-5 concrete security risks or best-practice gaps in short bullet points:\\\\n\\\\n\"\n",
        "        + text[:6000]\n",
        "    )\n",
        "    code, out, _ = run([\"ollama\", \"run\", \"llama3.1\", prompt])\n",
        "    return out if code == 0 else None\n",
        "\n",
        "# ----------------------------\n",
        "# Dataset assembly\n",
        "# ----------------------------\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--repo\", required=True, help=\"Path to repo root\")\n",
        "    ap.add_argument(\"--out\", default=\"coverage_dataset.csv\", help=\"Output CSV path\")\n",
        "    ap.add_argument(\"--prefer-tools\", action=\"store_true\", help=\"Use external scanners if present\")\n",
        "    ap.add_argument(\"--llm\", action=\"store_true\", help=\"Use ollama to summarize risks if installed\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    repo = Path(args.repo).resolve()\n",
        "    rows = []\n",
        "\n",
        "    # Gather artifacts\n",
        "    dockerfiles = [Path(root)/f for root,_,files in os.walk(repo) for f in files if f.lower().startswith(\"dockerfile\")]\n",
        "    manifests = []\n",
        "    for root,_,files in os.walk(repo):\n",
        "        for f in files:\n",
        "            lf = f.lower()\n",
        "            if lf in (\"requirements.txt\", \"package.json\", \"yarn.lock\", \"pnpm-lock.yaml\", \"poetry.lock\", \"pyproject.toml\", \"go.mod\", \"pom.xml\"):\n",
        "                manifests.append(Path(root)/f)\n",
        "\n",
        "    # Analyze Dockerfiles\n",
        "    for dpath in dockerfiles:\n",
        "        module = top_level_module(dpath, repo)\n",
        "        text = load_text(dpath)\n",
        "        name = f\"Dockerfile:{dpath.relative_to(repo)}\"\n",
        "\n",
        "        lint_score = hadolint_findings(dpath) if (args.prefer_tools) else 0\n",
        "        heur_score = analyze_dockerfile_heuristic(text)\n",
        "        vuln_fs = trivy_fs_findings(repo) if args.prefer_tools else 0  # repo-wide baseline\n",
        "        secret_score = 0  # placeholder; could add gitleaks/trufflehog\n",
        "\n",
        "        risk = combine_risk(vuln_fs, lint_score + heur_score, secret_score, 0)\n",
        "        sloc = sloc_count(text)\n",
        "        cc = None  # no CC for Dockerfile\n",
        "        complexity = combine_complexity(sloc, cc)\n",
        "\n",
        "        # test heuristic\n",
        "        tests = guess_tests_for_path(dpath, repo)\n",
        "        status = infer_status(tests, sloc)\n",
        "\n",
        "        priority = infer_priority(name, module)\n",
        "        impact = infer_impact(name, module)\n",
        "\n",
        "        rows.append([name, module, priority, round(risk,2), round(complexity,2), status, impact])\n",
        "\n",
        "        if args.llm:\n",
        "            summary = ollama_summarize(text)\n",
        "            if summary:\n",
        "                # write sidecar note\n",
        "                note = (repo / f\".dataset_notes_{dpath.name}.txt\")\n",
        "                try:\n",
        "                    note.write_text(summary, encoding=\"utf-8\")\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    # Analyze manifests (dependencies)\n",
        "    dep_score_repo = osv_findings(repo) if args.prefer_tools else 0\n",
        "    for mpath in manifests:\n",
        "        module = top_level_module(mpath, repo)\n",
        "        text = load_text(mpath)\n",
        "        name = f\"Deps:{mpath.relative_to(repo)}\"\n",
        "\n",
        "        vuln_score = dep_score_repo  # repo-wide OSV results\n",
        "        lint_score = 0\n",
        "        secrets = 0\n",
        "        misc = 0\n",
        "\n",
        "        risk = combine_risk(vuln_score, lint_score, secrets, misc)\n",
        "        sloc = sloc_count(text)\n",
        "        complexity = combine_complexity(sloc, None)\n",
        "        tests = guess_tests_for_path(mpath, repo)\n",
        "        status = infer_status(tests, sloc)\n",
        "\n",
        "        priority = infer_priority(name, module)\n",
        "        impact = infer_impact(name, module)\n",
        "\n",
        "        rows.append([name, module, priority, round(risk,2), round(complexity,2), status, impact])\n",
        "\n",
        "    # Fallback: add one row per top-level module folder with heuristics\n",
        "    top_levels = {p for p in (p for p in repo.iterdir() if p.is_dir() and not p.name.startswith(\".\"))}\n",
        "    for mod in top_levels:\n",
        "        files = list(mod.rglob(\"*.*\"))\n",
        "        total_sloc = 0\n",
        "        for fp in files:\n",
        "            total_sloc += sloc_count(load_text(fp))\n",
        "        cc = radon_cc_avg(repo, mod.name) if args.prefer_tools else None\n",
        "        tests = guess_tests_for_path(mod, repo)\n",
        "        # risk from trivy/semgrep across repo as a baseline signal\n",
        "        vuln = (trivy_fs_findings(repo) + semgrep_findings(repo)) if args.prefer_tools else 0\n",
        "        risk = combine_risk(vuln, 0, 0, 0)\n",
        "        complexity = combine_complexity(total_sloc, cc)\n",
        "\n",
        "        name = f\"Module:{mod.name}\"\n",
        "        module = mod.name\n",
        "        priority = infer_priority(name, module)\n",
        "        impact = infer_impact(name, module)\n",
        "        status = infer_status(tests, total_sloc)\n",
        "\n",
        "        rows.append([name, module, priority, round(risk,2), round(complexity,2), status, impact])\n",
        "\n",
        "    # Write CSV\n",
        "    import csv\n",
        "    out_path = Path(args.out).resolve()\n",
        "    with open(out_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"name\",\"module\",\"priority\",\"risk_score\",\"complexity_score\",\"status\",\"business_impact\"])\n",
        "        writer.writerows(rows)\n",
        "\n",
        "    print(f\"Wrote dataset with {len(rows)} rows to {out_path}\")\n",
        "    if not (which(\"hadolint\") or which(\"trivy\") or which(\"osv-scanner\") or which(\"semgrep\")):\n",
        "        print(\"Tip: Install scanners (hadolint, trivy, osv-scanner, semgrep) for richer, more realistic risk signals.\")\n",
        "    if rows == []:\n",
        "        print(\"No artifacts found. Ensure the repo path is correct and contains code or Dockerfiles/manifests.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\")\n",
        "\n",
        "train_and_save_py = dedent(\"\"\"\n",
        "#!/usr/bin/env python3\n",
        "\\\"\\\"\\\"\n",
        "train_and_save.py\n",
        "----------------------------------\n",
        "Trains multiple models on the generated coverage dataset and saves artifacts:\n",
        "- Supervised classifiers: DecisionTree, RandomForest, XGBoost\n",
        "- Unsupervised clustering: KMeans\n",
        "- Scaler + LabelEncoder\n",
        "\n",
        "USAGE:\n",
        "  python train_and_save.py --csv coverage_dataset.csv\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "import argparse\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--csv\", required=True, help=\"Path to coverage dataset CSV\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    df = pd.read_csv(args.csv)\n",
        "\n",
        "    # Encode categories\n",
        "    priority_map = {\"high\": 3, \"medium\": 2, \"low\": 1}\n",
        "    impact_map = {\"Critical\": 5, \"High\": 4, \"Medium\": 3, \"Low\": 2}\n",
        "\n",
        "    df[\"priority_num\"] = df[\"priority\"].map(priority_map).fillna(2)\n",
        "    df[\"impact_num\"] = df[\"business_impact\"].map(impact_map).fillna(3)\n",
        "\n",
        "    # Features: DO NOT include status-derived signal to avoid leakage\n",
        "    X = df[[\"priority_num\", \"risk_score\", \"complexity_score\", \"impact_num\"]].astype(float)\n",
        "    y = df[\"status\"].astype(str)\n",
        "\n",
        "    # Label encode target\n",
        "    le = LabelEncoder()\n",
        "    y_enc = le.fit_transform(y)\n",
        "\n",
        "    n_classes = len(np.unique(y_enc))\n",
        "    # For tiny datasets, set test_size to 0.5 for more balanced splits\n",
        "    test_size = 0.2 if len(df) > 12 else 0.5\n",
        "\n",
        "    # Ensure all classes are present in both sets\n",
        "    for _ in range(10):\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y_enc, test_size=test_size, random_state=np.random.randint(0, 10000), stratify=y_enc\n",
        "        )\n",
        "        if len(np.unique(y_train)) == n_classes and len(np.unique(y_test)) >= 1:\n",
        "            break\n",
        "    else:\n",
        "        raise ValueError(\"Could not create a split with all classes present in train. Please add more data.\")\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_s = scaler.fit_transform(X_train)\n",
        "    X_test_s = scaler.transform(X_test)\n",
        "\n",
        "    # Models\n",
        "    dtree = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "    dtree.fit(X_train, y_train)\n",
        "\n",
        "    rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    # Default to multi:softprob for >2 classes, otherwise binary:logistic\n",
        "    xgb = None\n",
        "    if n_classes > 1 and len(X_train) > 1:\n",
        "        try:\n",
        "            xgb = XGBClassifier(\n",
        "                n_estimators=300, max_depth=6, learning_rate=0.1,\n",
        "                subsample=0.8, colsample_bytree=0.8, random_state=42,\n",
        "                eval_metric=\"mlogloss\",\n",
        "                objective=\"multi:softprob\" if n_classes > 2 else \"binary:logistic\",\n",
        "                use_label_encoder=False\n",
        "            )\n",
        "            xgb.fit(X_train, y_train)\n",
        "        except Exception as e:\n",
        "            print(f\"XGBoost training failed: {e}\")\n",
        "            xgb = None\n",
        "    else:\n",
        "        print(\"Skipping XGBoost: not enough samples or only one class present in train set.\")\n",
        "\n",
        "        # Evaluate\n",
        "    for name, model in [(\"Decision Tree\", dtree), (\"Random Forest\", rf)] + ([(\"XGBoost\", xgb)] if xgb else []):\n",
        "        y_pred = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        print(f\"{name} Accuracy: {acc:.3f}\")\n",
        "        print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "        if hasattr(model, \"feature_importances_\"):\n",
        "            print(f\"Feature importances for {name}:\")\n",
        "            for col, imp in zip([\"priority\",\"risk\",\"complexity\",\"impact\"], model.feature_importances_):\n",
        "                print(f\"  {col}: {imp:.3f}\")\n",
        "\n",
        "    # Clustering on scaled features\n",
        "    kmeans = KMeans(n_clusters=min(n_classes, 3), random_state=42, n_init=10)\n",
        "    kmeans.fit(X_train_s)\n",
        "\n",
        "    # Save artifacts\n",
        "    with open(\"scaler.pkl\", \"wb\") as f: pickle.dump(scaler, f)\n",
        "    with open(\"label_encoder.pkl\", \"wb\") as f: pickle.dump(le, f)\n",
        "    with open(\"decision_tree.pkl\", \"wb\") as f: pickle.dump(dtree, f)\n",
        "    with open(\"random_forest.pkl\", \"wb\") as f: pickle.dump(rf, f)\n",
        "    if xgb is not None:\n",
        "        with open(\"xgboost.pkl\", \"wb\") as f: pickle.dump(xgb, f)\n",
        "    with open(\"kmeans_model.pkl\", \"wb\") as f: pickle.dump(kmeans, f)\n",
        "\n",
        "    print(\"\\\\nModels saved: scaler.pkl, label_encoder.pkl, decision_tree.pkl, random_forest.pkl, \" + (\"xgboost.pkl, \" if xgb else \"\") + \"kmeans_model.pkl\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\")\n",
        "\n",
        "(base / \"generate_dataset.py\").write_text(generate_dataset_py, encoding=\"utf-8\")\n",
        "(base / \"train_and_save.py\").write_text(train_and_save_py, encoding=\"utf-8\")\n",
        "\n",
        "print(\"Files created in /content/: generate_dataset.py, train_and_save.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2d7gd8-Hdfc",
        "outputId": "413cf405-d60c-47d4-c17e-ab39f9b161ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files created in /content/: generate_dataset.py, train_and_save.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/generate_dataset.py --repo /content/repo --out /content/coverage_dataset.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTotE6liHiEE",
        "outputId": "7c29e025-94bc-4263-bcf1-ac5592247e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote dataset with 16 rows to /content/coverage_dataset.csv\n",
            "Tip: Install scanners (hadolint, trivy, osv-scanner, semgrep) for richer, more realistic risk signals.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/train_and_save.py --csv /content/coverage_dataset.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEutqmVGHk_F",
        "outputId": "6148a206-2410-44c1-c202-33ea8cdd32c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [21:24:41] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "Decision Tree Accuracy: 1.000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     covered       1.00      1.00      1.00         1\n",
            "   uncovered       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           1.00         4\n",
            "   macro avg       1.00      1.00      1.00         4\n",
            "weighted avg       1.00      1.00      1.00         4\n",
            "\n",
            "Feature importances for Decision Tree:\n",
            "  priority: 0.000\n",
            "  risk: 0.000\n",
            "  complexity: 1.000\n",
            "  impact: 0.000\n",
            "Random Forest Accuracy: 1.000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     covered       1.00      1.00      1.00         1\n",
            "   uncovered       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           1.00         4\n",
            "   macro avg       1.00      1.00      1.00         4\n",
            "weighted avg       1.00      1.00      1.00         4\n",
            "\n",
            "Feature importances for Random Forest:\n",
            "  priority: 0.232\n",
            "  risk: 0.000\n",
            "  complexity: 0.556\n",
            "  impact: 0.213\n",
            "XGBoost Accuracy: 1.000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     covered       1.00      1.00      1.00         1\n",
            "   uncovered       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           1.00         4\n",
            "   macro avg       1.00      1.00      1.00         4\n",
            "weighted avg       1.00      1.00      1.00         4\n",
            "\n",
            "Feature importances for XGBoost:\n",
            "  priority: 0.531\n",
            "  risk: 0.000\n",
            "  complexity: 0.469\n",
            "  impact: 0.000\n",
            "\n",
            "Models saved: scaler.pkl, label_encoder.pkl, decision_tree.pkl, random_forest.pkl, xgboost.pkl, kmeans_model.pkl\n"
          ]
        }
      ]
    }
  ]
}